{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec47058b",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Build an automated ETL pipeline to process the e-commerce sales data (dataset1.csv), validate it, store it in a SQLite database with a structured schema, and generate an interactive dashboard to analyze sales and profit trends.\n",
    "\n",
    "Why more difficult?\n",
    "\n",
    "Automation: Write reusable functions to process data automatically.\n",
    "Data Validation: Check data quality (e.g., negative sales, invalid dates).\n",
    "Modular Code: Organize code into functions for scalability.\n",
    "Interactive Dashboard: Use Plotly for a web-based dashboard.\n",
    "ETL Focus: Mimic real-world data engineering workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564017f",
   "metadata": {},
   "source": [
    "# Define the Objective\n",
    "\n",
    "Build an ETL pipeline to:\n",
    "\n",
    "- Extract data from dataset1.csv.\n",
    "- Transform it (clean, validate, enrich).\n",
    "- Load it into a SQLite database.\n",
    "- Create an interactive dashboard showing sales and profit trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9262a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4df6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to track progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a51d6a4",
   "metadata": {},
   "source": [
    "# Write the ETL Pipeline (Extract)\n",
    "\n",
    "- What: Load the data and validate it.\n",
    "- Why: Ensure the data is reliable before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa13828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file_path):\n",
    "    \"Load CSV file and validate basic structure\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logging.info(f\"Loaded {len(df)} row from {file_path}\")\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Dataset is empty\")\n",
    "        if not all (col in df.columns for col in ['Order_Date', 'Sales', 'Profit', 'Product_Category']):\n",
    "            raise ValueError(\"Required columns missing\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error Loading Data {e}\")\n",
    "        raise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468259cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the extract function\n",
    "# df = extract_data('dataset1.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b45d6",
   "metadata": {},
   "source": [
    "# What’s new?\n",
    "\n",
    "- Logging: Tracks progress and errors (useful for debugging).\n",
    "- Validation: Checks for empty data or missing key columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17d696",
   "metadata": {},
   "source": [
    "# Transform the Data\n",
    "\n",
    "- What: Clean, validate, and enrich the data.\n",
    "- Why: To ensure quality and add useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03d984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"Clean, validate, and enrich the dataset.\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Remove Missing values\n",
    "        df_clean.dropna()\n",
    "        logging.info(f\"Remove Null Values\")\n",
    "        \n",
    "        # Convert data types\n",
    "        df_clean['Order_Date'] = pd.to_datetime(df_clean['Order_Date'])\n",
    "        categorical_Cols = ['Gender', 'Product_Category', 'Order_Priority', 'Payment_method']\n",
    "        for col in categorical_Cols:\n",
    "            df_clean[col] = df_clean[col].astype('category')\n",
    "            \n",
    "        # Validate numerical data\n",
    "        if (df_clean['Sales'] < 0).any():\n",
    "            logging.warning(\"Negative sales detected, setting to 0\")\n",
    "            df_clean['Sales'] = df_clean['Sales'].clip(lower=0)\n",
    "        if (df_clean['Profit'] < 0).any():\n",
    "            logging.warning(\"Negative profit detected, setting to 0\")\n",
    "            df_clean['Profit'] = df_clean['Profit'].clip(lower=0)\n",
    "            \n",
    "        # Remove duplicates\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        logging.info(f\"Removed {len(df) - len(df_clean)} duplicates\")\n",
    "        \n",
    "        # Enrich: Add year, month, and profit margin\n",
    "        df_clean['Year'] = df_clean['Order_Date'].dt.year\n",
    "        df_clean['Month'] = df_clean['Order_Date'].dt.month\n",
    "        df_clean['Profit_Margin'] = df_clean['Profit'] / df_clean['Sales'] * 100\n",
    "        \n",
    "        if df_clean['Order_Date'].isnull().any():\n",
    "            logging.warning(\"Invalid dates detected, dropping rows\")\n",
    "            df_clean = df_clean.dropna(subset=['Order_Date'])\n",
    "        logging.info(f\"Transformed data: {len(df_clean)} rows\")\n",
    "        return df_clean\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error transforming data: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc3691",
   "metadata": {},
   "source": [
    "# any()\n",
    "\n",
    "any() checks if at least one value in a list, array, or Pandas Series is True. In this case, it’s used to check if there are any negative values in the Sales or Profit columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa63e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the transform function\n",
    "# df_transformed = transform_data(df)\n",
    "# df_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a950c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transformed[['Sales', 'Profit', 'Profit_Margin']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24439483",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "- What: Store the transformed data in a SQLite database.\n",
    "- Why: To practice data storage, a core data engineering skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f17b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8306a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, db_name, table_name):\n",
    "    \"\"\"Save data to SQLite database.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        # Define schema explicitly\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Saved {len(df)} rows to {db_name}.{table_name}\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de0d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Load function\n",
    "# load_data(df_transformed, 'sales_data.db', 'sales_cleaned')\n",
    "# # Verify\n",
    "# conn = sqlite3.connect('sales_data.db')\n",
    "# test_df = pd.read_sql('SELECT * FROM sales_cleaned', conn)\n",
    "# print(test_df.head())\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccae63f",
   "metadata": {},
   "source": [
    "# Automate the Pipeline\n",
    "\n",
    "- What: Combine extract, transform, and load into a single pipeline.\n",
    "- Why: Automation is a key data engineering skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7294bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(file_path, db_name, table_name):\n",
    "    \"\"\"Run the full ETL pipeline.\"\"\"\n",
    "    logging.info(\"Starting ETL pipeline\")\n",
    "    df = extract_data(file_path)\n",
    "    df_transformed = transform_data(df)\n",
    "    load_data(df_transformed, db_name, table_name)\n",
    "    logging.info(\"ETL pipeline completed\")\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4adb95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 10:58:49,081 - INFO - Starting ETL pipeline\n",
      "2025-04-20 10:58:49,214 - INFO - Loaded 51290 row from dataset1.csv\n",
      "2025-04-20 10:58:49,247 - INFO - Remove Null Values\n",
      "2025-04-20 10:58:49,342 - INFO - Removed 0 duplicates\n",
      "2025-04-20 10:58:49,352 - INFO - Transformed data: 51290 rows\n",
      "2025-04-20 10:58:49,870 - INFO - Saved 51290 rows to sales_data.db.sales_cleaned\n",
      "2025-04-20 10:58:49,872 - INFO - ETL pipeline completed\n"
     ]
    }
   ],
   "source": [
    "df_final = run_pipeline('dataset1.csv', 'sales_data.db', 'sales_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162af0ac",
   "metadata": {},
   "source": [
    "# Create an Interactive Dashboard\n",
    "\n",
    "- What: Build a Plotly dashboard to visualize trends.\n",
    "- Why: To share insights interactively, a valuable skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b57825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3954518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dashboard(df):\n",
    "    \"Create an interactive dashboard with sales and profit trends.\"\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=2, cols=2, \n",
    "                        subplot_titles=(\"Sales by Month\", \"Profit by Product Category\", \"Profit Margin Distribution\", \"Discount vs. Profit\"))\n",
    "\n",
    "    # Plot 1: Sales by month\n",
    "    sales_by_month = df.groupby('Month')['Sales'].sum().reset_index()\n",
    "    trace1 = go.Scatter(x=sales_by_month['Month'], y=sales_by_month['Sales'], mode='lines+markers')\n",
    "    fig.add_trace(trace1, row=1, col=1)\n",
    "\n",
    "    # Plot 2: Profit by product category\n",
    "    profit_by_category = df.groupby('Product_Category', observed=True)['Profit'].mean().reset_index()\n",
    "    trace2 = go.Bar(x=profit_by_category['Product_Category'], y=profit_by_category['Profit'])\n",
    "    fig.add_trace(trace2, row=1, col=2)\n",
    "\n",
    "    # Plot 3: Profit margin distribution\n",
    "    trace3 = go.Histogram(x=df['Profit_Margin'], nbinsx=20)\n",
    "    fig.add_trace(trace3, row=2, col=1)\n",
    "\n",
    "    # Plot 4: Discount vs. profit\n",
    "    discount_profit = df.groupby('Discount')['Profit'].mean().reset_index()\n",
    "    trace4 = go.Bar(x=discount_profit['Discount'], y=discount_profit['Profit'], marker_color='yellow')\n",
    "    fig.add_trace(trace4, row=2, col=2)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text=\"Sales and Profit Dashboard\", showlegend=False, height=800)\n",
    "    fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06947e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to generate dashboard...\n",
      "Dashboard rendering complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting to generate dashboard...\")\n",
    "plot_dashboard(df_final)\n",
    "print(\"Dashboard rendering complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee47485",
   "metadata": {},
   "source": [
    "# What’s new?\n",
    "\n",
    "- Interactive Plots: Click, zoom, and hover to explore data.\n",
    "- Multiple Visuals: Combines four plots in one dashboard.\n",
    "- Plotly: Industry-standard tool for dashboards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
